\documentclass[../users_guide.tex]{subfiles}
 
\begin{document}

\section{Using the DAOS VOL connector within an HDF5 application}

This section outlines the unique aspects of writing, building and running
\acrshort{hdf5} applications with the \dvc{}.

\subsection{Building the HDF5 DAOS VOL connector}

The following is a quick set of instructions for building the \dvc{} connector.
Note that these instructions are not comprehensive and may be subject to change
in future releases; please refer to the \dvc{}'s
\href{https://bitbucket.hdfgroup.org/projects/HDF5VOL/repos/daos-vol/browse/README.md}{README} file for the most up to date instructions.

The \dvc{} is built using CMake. CMake version 2.8.12.2 or greater is required
to build the connector itself, but version 3.1 or greater is required to build
the connector's tests. To build the connector, one should create a build
directory within the source tree:

\begin{verbatim}
cd daos-vol
mkdir build
cd build
\end{verbatim}

After that, if all of the required components (DAOS, CaRT, MPI and HDF5) are
located within the system path, building the connector should be as simple as
running the following two commands to have CMake generate the build files for
\texttt{make} to use.

\begin{verbatim}
ccmake ..
make && make install
\end{verbatim}

Some notable CMake variables are listed below. These can be used to control
the build process and can be supplied to the \texttt{cmake} command by
prepending them with \texttt{-D} or turned on in \texttt{ccmake}. Some of the
connector-specific options may be needed if the required components mentioned
previously cannot be found within the system path.

CMake-specific options:

\begin{itemize}
  \item \texttt{CMAKE\_INSTALL\_PREFIX} --- This variable controls the install directory that the resulting output files are written to.
  \item \texttt{CMAKE\_BUILD\_TYPE} --- This variable controls the type of build used for the VOL connector. Valid values are Release, Debug, RelWithDebInfo and MinSizeRel. (\textit{Default: RelWithDebInfo})
\end{itemize}

\dvc{}-specific options:

\begin{itemize}
  \item \texttt{BUILD\_TESTING} --- This variable is used to enable/disable building of the \dvc{}'s tests.
  \item \texttt{BUILD\_EXAMPLES} --- This variable is used to enable/disable building of the DAOS VOL connector's HDF5 examples.
  \item \texttt{CART\_INCLUDE\_DIR} --- This variable controls the CaRT include directory used by the VOL connector build process. Used in conjunction with the \texttt{CART\_LIBRARY} variable.
  \item \texttt{CART\_LIBRARY} --- This variable controls the CaRT library used by the VOL connector build process. It should be set to the full path to the CaRT library, including the library's name (e.g., /path/libcart.so). Used in conjunction with the \texttt{CART\_INCLUDE\_DIR} variable.
  \item \texttt{DAOS\_LIBRARY} --- This variable controls the DAOS library used by the VOL connector build process. It should be set to the full path to the DAOS library, including the library's name (e.g., /path/libdaos.so). Used in conjunction with the \texttt{DAOS\_COMMON\_LIBRARY} and \texttt{DAOS\_INCLUDE\_DIR} variables.
  \item \texttt{DAOS\_COMMON\_LIBRARY} --- This variable controls the DAOS 'common' library used by the VOL connector build process. It should be set to the full path to the DAOS common library, including the library's name (e.g., /path/libdaos\_common.so). Used in conjunction with the \texttt{DAOS\_LIBRARY} and \texttt{DAOS\_INCLUDE\_DIR} variables.
  \item \texttt{DAOS\_INCLUDE\_DIR} --- This variable controls the DAOS include directory used by the VOL connector build process. Used in conjunction with the \texttt{DAOS\_LIBRARY} and \texttt{DAOS\_COMMON\_LIBRARY} variables.
  \item \texttt{MPI\_C\_COMPILER} --- This variable controls the MPI C Compiler used by the VOL connector build process. It should be set to the full path to the MPI C Compiler, including the name of the executable.
  \item \texttt{HDF5\_C\_COMPILER\_EXECUTABLE} --- This variable controls the HDF5 compiler wrapper script used by the VOL connector build process. It should be set to the full path to the HDF5 compiler wrapper, including the name of the wrapper script. The following two variables may also need to be set.
  \item \texttt{HDF5\_C\_LIBRARY\_hdf5} --- This variable controls the HDF5 library used by the VOL connector build process. It should be set to the full path to the HDF5 library, including the library's name (e.g., /path/libhdf5.so). Used in conjunction with the \texttt{HDF5\_C\_INCLUDE\_DIR} variable.
  \item \texttt{HDF5\_C\_INCLUDE\_DIR} --- This variable controls the HDF5 include directory used by the VOL connector build process. Used in conjunction with the \texttt{HDF5\_C\_LIBRARY\_hdf5} variable.
\end{itemize}

\subsection{Using the HDF5 DAOS VOL connector with applications}

There are currently three ways to tell an \acrshort{hdf5} application to use
the \dvc{}: 

\begin{enumerate}
    \item The \texttt{HDF5\_VOL\_CONNECTOR} environment variable
    \item \texttt{H5Pset\_vol()}, an \acrshort{hdf5} API call
    \item \texttt{H5Pset\_fapl\_daos()}, a \dvc{} API call
\end{enumerate}

Which option to choose will depend on things like the ability to modify the
application's source code, whether dynamically loading plugins is an option,
etc.

The environment variable is useful when the application either cannot or
does not need to be modified to use DAOS storage (e.g., the application makes
no native-specific \acrshort{hdf5} API calls). It is also the easiest to use
as it requires very little setup and no changes to the application's source
code. It is probably not going to be suitable, however, if the application
needs to perform I/O with multiple connectors. For example, reading from DAOS
and writing to the native \vc{} would be a problem. In the \acrshort{hdf5}
command-line tools, this difficulty is solved by using command-line parameters
and explicitly specifying a connector via \texttt{H5Pset\_vol()}.

Using \texttt{H5Pset\_vol()} requires modification of the application's
source code, which may not be possible. It's more flexible,
however, and allows an application to be configured to use VOL connectors in
arbitrary ways. Since it's an \acrshort{hdf5} API call, it isn't necessary to
link to the \dvc{}, which can be loaded as a plugin in the same way as using
the environment variable, described above.

\texttt{H5Pset\_fapl\_daos()} will give the application more control over how
DAOS is initialized, but it requires linking to the \dvc{} and modification of
the application's source code.

Each of these ways of specifying and loading the \dvc{} is fundamentally
equivalent in terms of its eventual effects. There's no performance or
functionality changes between them.

\subsubsection{Locating and loading HDF5 plugins at runtime}

\acrshort{hdf5} plugins are discovered at runtime by searching in specified
plugin paths. There is a library default plugin path (usually
\texttt{/usr/local/hdf5/lib/plugin} on POSIX systems) but this can be
overridden using the \texttt{HDF5\_PLUGIN\_PATH} environment variable (and/or
manipulated via the various \acrshort{hdf5} \texttt{H5PL} API calls for more
complicated situations where the application's source code can be modified).

When the \acrshort{hdf5} library is asked to use a plugin it doesn't
currently have loaded, it searches the plugin path(s), attempting to load
any plugins it finds to see if their name or connector value match.

\subsubsection{Using the HDF5\_VOL\_CONNECTOR environment variable}

Set the \texttt{HDF5\_VOL\_CONNECTOR} environment variable to "daos"
(case-sensitive) and make sure the \dvc{} plugin is discoverable, as
described above.

\subsubsection{Using H5Pset\_vol()}

The first thing that must be done is load the \dvc{} plugin via \newline
\texttt{H5VL\_register\_connector\_by\_(value|name)()}. This will load the
plugin and assign it a \texttt{hid\_t} connector ID. This ID is then passed
as the \vc{} ID when you call \texttt{H5Pset\_vol()} on the file access
property list that will be used to open the file.

The \dvc{} value is currently 4004. This is documented in the connector's
public header. Note that the connector value is unrelated to the
\texttt{hid\_t} library ID obtained from the registration API call.

\subsubsection{Using H5Pset\_daos\_vol()}

If dynamic loading of the \dvc{} is not used, any \acrshort{hdf5} application
using the connector must:
\begin{enumerate}
 \item Include \texttt{daos\_vol\_public.h}, found in the \texttt{include}
directory of the \dvc{} installation directory.
 \item Link against \texttt{libhdf5\_vol\_daos.so} (or similar), found in
the \texttt{lib} directory of the \dvc{} installation directory, and
against \texttt{libuuid.so} (or similar) in order to use UUIDs. Note that dependencies
can alternatively be retrieved through CMake or pkg-config.
\end{enumerate}

An \acrshort{hdf5} \dvc{} application also requires in that particular case three new
function calls in addition to those for an equivalent \acrshort{hdf5} application (see
Appendix~\ref{apdx:ref_manual} for more details):

\begin{itemize}
 \item \texttt{\hyperref[ref:h5daos_init]{H5daos\_init()}} --- Initializes the \dvc{}

    Called upon application startup, before any file is accessed.

 \item \texttt{\hyperref[ref:h5pset_fapl_daos]{H5Pset\_fapl\_daos()}} --- Sets \dvc{} access on File Access Property List.

    Called to prepare a FAPL to open a file through the \dvc{}. See \href{https://support.hdfgroup.org/HDF5/Tutor/property.html#fa}{HDF5 File Access Property Lists} for more information about File Access Property Lists.

 \item \texttt{\hyperref[ref:h5daos_term]{H5daos\_term()}} --- Cleanly shutdowns the \dvc{}

    Called on application shutdown, after all files have been closed.
\end{itemize}

\subsubsection{Skeleton Example}

Below is a no-op application that opens and closes a file using the \dvc{}.
For clarity, no error-checking is performed. Note that this example is
meant only for the case when the \dvc{} is not being dynamically loaded.

\begin{minted}[breaklines=true,fontsize=\small]{hdf5-c-lexer.py:HDF5CLexer -x}
#include "hdf5.h"
#include "daos_vol_public.h"

int main(void)
{
    uuid_t pool_uuid;
    hid_t fapl_id, file_id;

    /* Parse the pool UUID. */
    uuid_parse("fce30f79-b34b-46c1-9b1f-bb52d99dacca", pool_uuid);

    /* Initialize DAOS VOL connector using the above parsed UUID for
     * the pool UUID, "daos_server" as the group name for the DAOS
     * servers managing the pool and simply rank 0 as the only rank
     * in the pool service list. */
    H5daos_init(pool_uuid, "daos_server", "0");

    fapl_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_daos(fapl_id, MPI_COMM_WORLD, MPI_INFO_NULL);

    /* Currently required for the DAOS VOL connector, set all metadata
     * operations to be collective */
    H5Pset_all_coll_metadata_ops(fapl_id, true); 

    file_id = H5Fopen("my_file.h5", H5F_ACC_RDWR, fapl_id);

    /* Operate on file */
    [...]

    H5Pclose(fapl_id);
    H5Fclose(file_id);

    /* Terminate the DAOS VOL connector. */
    H5daos_term();

    return 0;
}
\end{minted}

\subsection{Asynchronous I/O}

The DAOS VOL connector supports asynchronous HDF5 operations using the HDF5
event set (H5ES) API, released in HDF5 1.13.0. This allows I/O to proceed in the
background while the application is performing other tasks. Documentation on the
H5ES API can be found in the
\href{https://portal.hdfgroup.org/display/HDF5/Asynchronous+operations+with+HDF5+VOL+connectors}{HDF5 Application Developer's Guide} and the
\href{https://portal.hdfgroup.org/display/HDF5/Event+Set}{HDF5 Reference Manual}.


\subsubsection{Implementation and making progress}

Asynchronous I/O in the DAOS VOL connector is implemented using a threadless
progress engine, that checks for completion of in-flight operations any time it
is entered. This means there is no background thread making progress, so you can
be certain it won't interfere with computation. However, this also means that
the connector needs to be entered occasionally in order to make progress, by
calling \mintcinline{H5ESwait()} with a timeout of \mintcinline{0}. Otherwise,
asynchronous operations will never complete until waited on, and the wait may
take a long time, no matter how much time has passed since the asynchronous
operation was issued.

Each HDF5 operation is split up into multiple internal tasks which may need to
be executed in a certain order. For example,s a group create needs to create a
link in the parent group, write the metadata to the new group, and possibly
adjust the number of links in the parent group and allocate an object ID for the
new group. The DAOS VOL connector uses the DAOS Thread Scheduling Engine (tse)
to track dependencies between these internal tasks and make progress on them,
together with custom code written to make progress on asynchronous MPI
operations needed for collective parallel operations.

\subsubsection {Consistency semantics}

Similarly to other asynchronous I/O libraries, the application must be careful
not to use, modify, or free any buffers in use by async tasks until those tasks
are complete. This applies to all read and query operations, as well as raw data
and attribute write operations.  For non-attribute metadata write operations,
the connector will make a temporary copy of any buffers passed in.

The application must also be careful not to assume write operations are visible
in the file until it has verified that the operation has completed through the
H5ES interface. For example, if you write to a dataset, you must wait for the
write to complete before reading that data from the dataset if you wish to see
the new data. Likewise, if you create a link, you must wait for the create to
complete before reading that link.

It is possible in some cases, however, to issue operations before prerequisites
have been complete. Any ID returned from the API can be passed back in through
the HDF5 API even if the open operation for the object that that ID refers to
has not completed. This allows applications to, for example, create a file,
create a group in the file, create a dataset in the group, write to the dataset,
and close all IDs, all in a non-blocking manner without waiting (until the
dataset write buffer needs to be modified or freed). Until the open operation is
complete, you can only access the object through its ID, so you cannot, for
example, create a group then create an object inside that group using a path
that includes the group name, without waiting for the first create to complete
(or using \mintcinline{H5Oflush_async()} or \mintcinline{H5Fflush_async()}, see
below).

As an example, this series of calls is legal:

\begin{verbatim}
group_id = H5Gcreate_async(file_id, "parent", ..., es_id);
dset_id = H5Dcreate_async(group_id, "dset", ..., es_id);
H5Dwrite_async(dset_id, ..., es_id);
â€¦
H5ESwait(es_id);
\end{verbatim}

Note that \mintcinline{H5Dcreate_async()} is called before we know
\mintcinline{H5Gcreate_async()} has completed, as with
\mintcinline{H5Dwrite_async()} and \mintcinline{H5Dcreate_async()}. However,
this example is illegal and may result in errors:

\begin{verbatim}
group_id = H5Gcreate_async(file_id, "parent", ..., es_id);
dset_id = H5Dcreate_async(file_id, "parent/dset", ..., es_id); //may fail
\end{verbatim}

The only difference between the traditional blocking HDF5 API calls and the
async versions is that, in the blocking versions, the connector waits for that
operation to complete before returning. Therefore, when mixing synchronous and
asynchronous calls, the application must still obey the above rules even when
calling a synchronous function - previously called asynchronous functions may
not be complete.

As an example, this series of calls is illegal and may result in an error:

\begin{verbatim}
group_id = H5Gcreate_async(file_id, "parent", ..., es_id);
dset_id = H5Dcreate(file_id, "parent/dset", ...); //may fail
\end{verbatim}

\subsubsection {Operation ordering}

For the most part, all asynchronous operations execute concurrently without any
ordering enforced betwen them. However, there are a few exceptions. The
connector enforces ordering between object open operations and operations that
use that object, in order to facilitate the feature described above that allows
the use of incompletely opened object IDs. In addition,
\mintcinline{H5Dset_extent(_async)()}, link/object creates when the parent group
has link creation order tracked, and attribute creates when the parent object
has attribute creation order tracked are always strictly ordered, so these
operations always execute after any previously issued operation related to their
object, and before any subsequently issued operations.

The application can manually enforce ordering using
\mintcinline{H5Oflush_async()} and \mintcinline{H5Fflush_async()}. These
operations only complete when all previously issued operations for the object or
file complete and, like \mintcinline{H5Dset_extent_async()}, all subsequently
issued operations only begin after the flush is complete. Since the DAOS HDF5
connector does no caching, the flush operations have no other effect. This
allows you to, for example, write to an attribute, issue an
\mintcinline{H5Oflush_async()} on the attribute's parent object, and read the
attribute back, all in a non-blocking fashion (as long as you keep both read and
write buffers around and don't examine the read buffer until it is complete).
See the Operation Scope section for information on which operations are in scope
for \mintcinline{H5Oflush_async}.

As an example, here is a way to make the previously illegal example legal while
remaining fully asynchronous:

\begin{verbatim}
group_id = H5Gcreate_async(file_id, "parent", ..., es_id);
H5Fflush_async(file_id, H5F_SCOPE_LOCAL, es_id);
dset_id = H5Dcreate_async(file_id, "parent/dset", ..., es_id);
\end{verbatim}

A call to \mintcinline{H5Oflush_async(group_id, es_id)} would not be sufficient
in this case because the group creation occurs in \mintcinline{file_id}'s scope,
because \mintcinline{file_id} is what is passed to
\mintcinline{H5Gcreate_async()}. The flush must be made on the parent object.

Close operations such as \mintcinline{H5Gclose()} or \mintcinline{H5Fclose()}
will only complete once all previously issued operations on their object, file
or attribute complete. Therefore, the non-async versions of these operations
will block until these operations are complete. Asynchronous versions of all
close operations are available for non-blocking close.

\subsubsection {Parallel Considerations}

Parallel collective operations add another constraint on asynchronous
operations. Because asynchronous MPI operations must be strictly ordered, all
collective HDF5 operations are strictly ordered with respect to each other when
executed with more than one rank, and no two can execute at the same time.
Non-collective operations are not affected by this, and may execute
simultaneouly with collective operations.

\subsubsection {Operation Scope}

In order to handle some operations that need to be ordered with respect to all
other operations within a certain scope, we have introduced the concept of an
operation pool, which is a container for operations that can be executed
simultaneously. All operations exist in an operation pool at attribute, object,
file, or global scope. When the application invokes an HDF5 API routine, this
creates an operation this is placed in the operation pool of the object passed
to the API routine. For example, dataset creates are placed in the object scoped
pool of the parent group passed as the first argument to
\mintcinline{H5Dcreate()}, attribute creates are placed in the object scoped
pool of the parent object passed as the first argument to
\mintcinline{H5Acreate()}, and attribute writes are placed in the attribute
scoped pool of the attribute. Operations that have multiple parent objects (such
as \mintcinline{H5Ocopy()}) are placed in the file pool if the objects are in
the same file, and in the global pool if they are in different files.

All operations placed in a pool will be executed after all previously issued
operations at a different scope in a location that contains, or is contained in,
the pool for the original operation. For example, an operation in an attribute's
pool will execute after all previously issued operations in the pools for the
attribute's parent object, its file, or the global pool. It is not affected by
operations in a different object or file. Operations in the global pool will
execute after all previously issued operations not in the global pool.

For the most part, this operation pool scheme is transparent to the application
and can be ignored. However, for maximum concurrency, it is worth considering
that certain access patterns will result operations being serialized, so only
one may execute at a time (though they will remain asynchronous). This will
happen if, for example, the app alternates between attribute and object
operations.

\subsubsection {Asynchronous Example Program}

Below is an example of an asynchronous application that writes to a dataset:

\begin{minted}[breaklines=true,fontsize=\small]{hdf5-c-lexer.py:HDF5CLexer -x}
#include "hdf5.h"
#include "daos_vol_public.h"

int main(void)
{
    hid_t file_id, group_id, dset_id, es_id;
    ... // Buffers, dataspaces, etc.
    herr_t status;

    es_id = H5EScreate();      // Create event set for tracking async operations

    file_id = H5Fopen_async("file.h5", H5F_ACC_RDWR, H5P_DEFAULT, es_id);
    group_id = H5Gopen_async(file_id, "parent", H5P_DEFAULT, es_id);
                                        // Starts when H5Fopen completes
    dset_id = H5Dopen_async(group_id, "dataset", H5P_DEFAULT, es_id);
                                        // Starts when H5Gopen completes

    status = H5Dwrite_async(dset_id, ..., es_id); // Asynchronous, starts when
                                              // H5Dopen completes, may run
                                              // concurrently with other
                                              // H5Dwrite in event set
    status = H5Dwrite_async(dset_id, ..., es_id); // Asynchronous, starts when
                                              // H5Dopen completes, may run
                                              // concurrently with other
                                              // H5Dwrite in event set

    status = H5Dclose_async(dset_id); // Asynchronous, will complete when both
                                      // writes above complete

    ...
    <other user code>
    ...
    H5ESwait(es_id); // Wait for operations in event set to complete, buffers
                     // used for H5Dwrite must not be changed until after wait
                     // returns
    ...

    status = H5Gclose(group_id); // Blocking call, will return as soon as all
                                 // operations on group_id complete
    status = H5Fclose(file_id);  // Blocking call, will return as soon as all
                                 // operations on file_id complete

    return 0;
}
\end{minted}

\subsection{Building HDF5 DAOS VOL connector applications}

Assuming an \acrshort{hdf5} application has been written following the instructions in the previous section, the application should be built as normal for any other
\acrshort{hdf5} application. However,
if the \dvc{} is not being dynamically loaded, the steps in the following
section are required to build the application.

\subsubsection{Without the DAOS VOL connector as a dynamically-loaded plugin}

To link in the required libraries, the compiler will likely require the
additional linker flags:

\begin{verbatim}
-lhdf5_vol_daos -luuid
\end{verbatim}

However, these flags may vary depending on platform, compiler and installation
location of the \dvc{}. It is highly recommended that compilation
of \acrshort{hdf5} \dvc{} applications be done using either the
\texttt{h5cc/h5pcc} script included with \acrshort{hdf5} distributions, or CMake,
pkg-config, as these will manage linking with the \acrshort{hdf5} library.

If \acrshort{hdf5} was built using autotools, this script will be called \texttt{h5pcc} and
may be found in the \texttt{bin} directory of the \acrshort{hdf5} installation. If \acrshort{hdf5}
was built with CMake, this script will simply be called \texttt{h5cc} and can
be found in the same location. The above notice about additional library
linking applies to usage of \texttt{h5cc/h5pcc}. For example:
\begin{verbatim}
h5cc/h5pcc -lhdf5_vol_daos -luuid my_application.c -o my_application
\end{verbatim}

\subsection{Running HDF5 DAOS VOL connector applications}
\label{running_daos_vol_apps}

Running applications that use the \dvc{} connector requires access to a \acrshort{daos}
server. Refer to
\href{https://daos-stack.github.io/admin/installation/}{DAOS Software Installation}
for more information on the setup process for this. For the \dvc{}
to correctly interact with a \acrshort{daos} server instance, the server must be \hyperref[sec:daos_serv_start]{running} and it must be passed the UUID of the
\acrshort{daos} pool to use and a list of \acrshort{daos} pool service list ranks, as detailed in the
following sections.

\subsubsection{Starting the DAOS Server}
\label{sec:daos_serv_start}

Instructions for starting a \acrshort{daos} Server can be found in the \href{https://daos-stack.github.io/admin/deployment/#server-startup}{DAOS Documentation}.

\subsubsection{With the DAOS VOL connector as a dynamically-loaded plugin}

If the \dvc{} is dynamically loaded by \acrshort{hdf5}, the \acrshort{daos} pool UUID and
\acrshort{daos} pool service rank list are passed via the two environment variables below.

\begin{verbatim}
DAOS_POOL - The UUID of the DAOS pool to use.

DAOS_SVCL - A comma-separated list of server ranks used for
            daos_pool_connect(). Generated from daos_pool_create().
\end{verbatim}

\subsubsection{Without the connector as a dynamically-loaded plugin}

If the \dvc{} is not being dynamically loaded, the \acrshort{daos} pool UUID
and \acrshort{daos} pool service rank list should be passed via the call to
\hyperref[ref:h5daos_init]{H5daos\_init()} within the application.

\subsubsection{Example Applications}

Some of the example C applications which are included with \acrshort{hdf5} distributions have been adapted to work with the \dvc{} and are included under the top-level \href{https://bitbucket.hdfgroup.org/projects/HDF5VOL/repos/daos-vol/browse/examples}{\texttt{examples}} directory in the \dvc{} source root directory. The built example applications can be run from the \texttt{bin} directory inside the build directory.

In addition to these examples, the \href{https://bitbucket.hdfgroup.org/projects/HDF5VOL/
repos/daos-vol/browse/test}{\texttt{test/vol}} directory contains several test
files, each containing test functions that are examples of \acrshort{hdf5} applications in
miniature, focused on a particular behavior. These mini-application tests cover a moderate 
amount of \acrshort{hdf5}'s public API functionality and should be a good indicator of
whether the \dvc{} is working correctly in conjunction with a
running \acrshort{daos} API-aware instance. Note that these tests currently rely on \acrshort{hdf5}'s
dynamically-loaded \vc{} capabilities in order to run with the \dvc{}.

\end{document}
