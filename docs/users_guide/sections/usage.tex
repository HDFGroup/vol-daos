\documentclass[../users_guide.tex]{subfiles}
 
\begin{document}

\section{Using the DAOS VOL connector within an HDF5 application}

This section outlines the unique aspects of writing, building and running
\acrshort{hdf5} applications with the \dvc{}.

\subsection{Building the HDF5 DAOS VOL connector}

The following is a quick set of instructions for building the \dvc{} connector.
Note that these instructions are not comprehensive and may be subject to change
in future releases; please refer to the \dvc{}'s
\href{https://bitbucket.hdfgroup.org/projects/HDF5VOL/repos/daos-vol/browse/README.md}{README} file for the most up to date instructions.

The \dvc{} is built using CMake. CMake version 2.8.12.2 or greater is required
to build the connector itself, but version 3.1 or greater is required to build
the connector's tests. To build the connector, one should create a build
directory within the source tree:

\begin{verbatim}
cd daos-vol
mkdir build
cd build
\end{verbatim}

After that, if all of the required components (DAOS, CaRT, MPI and HDF5) are
located within the system path, building the connector should be as simple as
running the following two commands to have CMake generate the build files for
\texttt{make} to use.

\begin{verbatim}
ccmake ..
make && make install
\end{verbatim}

Some notable CMake variables are listed below. These can be used to control
the build process and can be supplied to the \texttt{cmake} command by
prepending them with \texttt{-D} or turned on in \texttt{ccmake}. Some of the
connector-specific options may be needed if the required components mentioned
previously cannot be found within the system path.

CMake-specific options:

\begin{itemize}
  \item \texttt{CMAKE\_INSTALL\_PREFIX} --- This variable controls the install directory that the resulting output files are written to.
  \item \texttt{CMAKE\_BUILD\_TYPE} --- This variable controls the type of build used for the VOL connector. Valid values are Release, Debug, RelWithDebInfo and MinSizeRel. (\textit{Default: RelWithDebInfo})
\end{itemize}

\dvc{}-specific options:

\begin{itemize}
  \item \texttt{BUILD\_TESTING} --- This variable is used to enable/disable building of the \dvc{}'s tests.
  \item \texttt{BUILD\_EXAMPLES} --- This variable is used to enable/disable building of the DAOS VOL connector's HDF5 examples.
  \item \texttt{CART\_INCLUDE\_DIR} --- This variable controls the CaRT include directory used by the VOL connector build process. Used in conjunction with the \texttt{CART\_LIBRARY} variable.
  \item \texttt{CART\_LIBRARY} --- This variable controls the CaRT library used by the VOL connector build process. It should be set to the full path to the CaRT library, including the library's name (e.g., /path/libcart.so). Used in conjunction with the \texttt{CART\_INCLUDE\_DIR} variable.
  \item \texttt{DAOS\_LIBRARY} --- This variable controls the DAOS library used by the VOL connector build process. It should be set to the full path to the DAOS library, including the library's name (e.g., /path/libdaos.so). Used in conjunction with the \texttt{DAOS\_COMMON\_LIBRARY} and \texttt{DAOS\_INCLUDE\_DIR} variables.
  \item \texttt{DAOS\_COMMON\_LIBRARY} --- This variable controls the DAOS 'common' library used by the VOL connector build process. It should be set to the full path to the DAOS common library, including the library's name (e.g., /path/libdaos\_common.so). Used in conjunction with the \texttt{DAOS\_LIBRARY} and \texttt{DAOS\_INCLUDE\_DIR} variables.
  \item \texttt{DAOS\_INCLUDE\_DIR} --- This variable controls the DAOS include directory used by the VOL connector build process. Used in conjunction with the \texttt{DAOS\_LIBRARY} and \texttt{DAOS\_COMMON\_LIBRARY} variables.
  \item \texttt{MPI\_C\_COMPILER} --- This variable controls the MPI C Compiler used by the VOL connector build process. It should be set to the full path to the MPI C Compiler, including the name of the executable.
  \item \texttt{HDF5\_C\_COMPILER\_EXECUTABLE} --- This variable controls the HDF5 compiler wrapper script used by the VOL connector build process. It should be set to the full path to the HDF5 compiler wrapper, including the name of the wrapper script. The following two variables may also need to be set.
  \item \texttt{HDF5\_C\_LIBRARY\_hdf5} --- This variable controls the HDF5 library used by the VOL connector build process. It should be set to the full path to the HDF5 library, including the library's name (e.g., /path/libhdf5.so). Used in conjunction with the \texttt{HDF5\_C\_INCLUDE\_DIR} variable.
  \item \texttt{HDF5\_C\_INCLUDE\_DIR} --- This variable controls the HDF5 include directory used by the VOL connector build process. Used in conjunction with the \texttt{HDF5\_C\_LIBRARY\_hdf5} variable.
\end{itemize}

\subsection{Writing HDF5 DAOS VOL connector applications}

There are currently two main ways to tell an existing \acrshort{hdf5} application to use
the \dvc{}: either \textit{implicitly} by using environment
variables to tell the \acrshort{hdf5} library to load the connector as a dynamically loaded
plugin or \textit{explicitly} by making use of \acrshort{hdf5} property lists.

\subsubsection{With the DAOS VOL connector as a dynamically-loaded plugin}
\label{sec:dynamic_plugin}

\acrshort{hdf5} has the capability to dynamically load and use a \vc{} for running
applications with. In order to choose a particular \vc{} to use, two
initial steps must be taken. First, one must help \acrshort{hdf5} locate the \vc{}
by pointing to the directory which contains the built library. This can be
accomplished by setting the environment variable \texttt{HDF5\_PLUGIN\_PATH} to
this directory. Next, \acrshort{hdf5} needs to know the name of which library to use, which
is configured by setting the environment variable \texttt{HDF5\_VOL\_CONNECTOR}
to the name of the connector.

In order to use the \dvc{}, the aforementioned environment variables 
should be set as:

\begin{verbatim}
HDF5_PLUGIN_PATH=/daos/vol/installation/directory/lib
HDF5_VOL_CONNECTOR=daos
\end{verbatim}

Having completed this step, \acrshort{hdf5} will be setup to load the \dvc{}
and use it for running applications, including \acrshort{hdf5}'s own tests.
No additional modifications will need to be made to the existing \acrshort{hdf5} application.

\subsubsection{Without the DAOS VOL connector as a dynamically-loaded plugin}

If dynamic loading of the \dvc{} is not used, any \acrshort{hdf5} application
using the connector must:
\begin{enumerate}
 \item Include \texttt{daos\_vol\_public.h}, found in the \texttt{include}
directory of the \dvc{} installation directory.
 \item Link against \texttt{libhdf5\_vol\_daos.so} (or similar), found in
the \texttt{lib} directory of the \dvc{} installation directory, and
against \texttt{libuuid.so} (or similar) in order to use UUIDs. Note that dependencies
can alternatively be retrieved through CMake or pkg-config.
\end{enumerate}

An \acrshort{hdf5} \dvc{} application also requires in that particular case three new
function calls in addition to those for an equivalent \acrshort{hdf5} application (see
Appendix~\ref{apdx:ref_manual} for more details):

\begin{itemize}
 \item \texttt{\hyperref[ref:h5daos_init]{H5daos\_init()}} --- Initializes the \dvc{}

    Called upon application startup, before any file is accessed.

 \item \texttt{\hyperref[ref:h5pset_fapl_daos]{H5Pset\_fapl\_daos()}} --- Sets \dvc{} access on File Access Property List.

    Called to prepare a FAPL to open a file through the \dvc{}. See \href{https://support.hdfgroup.org/HDF5/Tutor/property.html#fa}{HDF5 File Access Property Lists} for more information about File Access Property Lists.

 \item \texttt{\hyperref[ref:h5daos_term]{H5daos\_term()}} --- Cleanly shutdowns the \dvc{}

    Called on application shutdown, after all files have been closed.
\end{itemize}

\subsubsection{Skeleton Example}

Below is a no-op application that opens and closes a file using the \dvc{}.
For clarity, no error-checking is performed. Note that this example is
meant only for the case when the \dvc{} is not being dynamically loaded.

\begin{minted}[breaklines=true,fontsize=\small]{hdf5-c-lexer.py:HDF5CLexer -x}
#include "hdf5.h"
#include "daos_vol_public.h"

int main(void)
{
    uuid_t pool_uuid;
    hid_t fapl_id, file_id;

    /* Parse the pool UUID. */
    uuid_parse("fce30f79-b34b-46c1-9b1f-bb52d99dacca", pool_uuid);

    /* Initialize DAOS VOL connector using the above parsed UUID for
     * the pool UUID, "daos_server" as the group name for the DAOS
     * servers managing the pool and simply rank 0 as the only rank
     * in the pool service list. */
    H5daos_init(pool_uuid, "daos_server", "0");

    fapl_id = H5Pcreate(H5P_FILE_ACCESS);
    H5Pset_fapl_daos(fapl_id, MPI_COMM_WORLD, MPI_INFO_NULL);

    /* Currently required for the DAOS VOL connector, set all metadata
     * operations to be collective */
    H5Pset_all_coll_metadata_ops(fapl_id, true); 

    file_id = H5Fopen("my_file.h5", H5F_ACC_RDWR, fapl_id);

    /* Operate on file */
    [...]

    H5Pclose(fapl_id);
    H5Fclose(file_id);

    /* Terminate the DAOS VOL connector. */
    H5daos_term();

    return 0;
}
\end{minted}

\subsection{Asynchronous I/O}

The DAOS VOL connector supports asynchronous HDF5 operations using the HDF5
event set (H5ES) API, released in HDF5 1.13.0. This allows I/O to proceed in the
background while the application is performing other tasks.

\subsubsection{Implementation and making progress}

Asynchronous I/O in the DAOS VOL connector is implemented using a threadless
progress engine, that checks for completion of in-flight operations any time it
is entered. This means there is no background thread making progress, so you can
be certain it won't interfere with computation. However, this also means that
the connector needs to be entered occasionally in order to make progress, by
calling \mintcinline{H5ESwait()} with a timeout of \mintcinline{0}. Otherwise,
asynchronous operations will never complete until waited on, and the wait may
take a long time, no matter how much time has passed since the asynchronous
operation was issued.

\subsubsection {Consistency semantics}

Similarly to other asynchronous I/O libraries, the application must be careful
not to use, modify, or free any buffers in use by async tasks until those tasks
are complete. This applies to all read and query operations, as well as raw data
and attribute write operations.  For non-attribute metadata write operations,
the connector will make a temporary copy of any buffers passed in.

The application must also be careful not to assume write operations are visible
in the file until it has verified that the operation has completed through the
H5ES interface. For example, if you write to a dataset, you must wait for the
write to complete before reading that data from the dataset if you wish to see
the new data. Likewise, if you create a link, you must wait for the create to
complete before reading that link.

It is possible in some cases, however, to issue operations before prerequisites
have been complete. Any ID returned from the API can be passed back in through
the HDF5 API even if the open operation for the object that that ID refers to
has not completed. This allows applications to, for example, create a file,
create a group in the file, create a dataset in the group, write to the dataset,
and close all IDs, all in a non-blocking manner without waiting (until the
dataset write buffer needs to be modified or freed). Keep in mind that links are
not included in this, so you cannot, for example, create a group then create an
object inside that group using a path that includes the group name, without
waiting for the first create to complete (or using
\mintcinline{H5Oflush_async()} or \mintcinline{H5Fflush_async()}, see below).

Keep in mind that the only difference between the traditional blocking HDF5 API
calls and the async versions is that the connector waits for that operation to
complete before returning. The application must still make sure the file is in
a consistent enough state to make the call before doing so, and must be aware
that any operations the call does not depend on may still be incomplete after
returning.

\subsubsection {Operation ordering}

For the most part, all asynchronous operations execute concurrently without any
ordering enforced betwen them. However, there are a few exceptions. The
connector enforces ordering between object open operations and operations that
use that object, in order to facilitate the feature described above that allows
the use of incompletely opened object IDs. In addition,
\mintcinline{H5Dset_extent(_async)()}, link/object creates when the parent group
has link creation order tracked, and attribute creates when the parent object
has attribute creation order tracked are always strictly ordered, so these
operations always execute after any previously issued operation related to their
object, and before any subsequently issued operations.

The application can manually enforce ordering using
\mintcinline{H5Oflush_async()} and \mintcinline{H5Fflush_async()}. These
operations only complete when all previously issued operations for the object or
file complete and, like \mintcinline{H5Dset_extent_async()}, all subsequently
issued operations only begin after the flush is complete. Since the DAOS HDF5
connector does no caching, the flush operations have no other effect. This
allows you to, for example, write to an attribute, issue an
\mintcinline{H5Oflush_async()} on the attribute's parent object, and read the
attribute back, all in a non-blocking fashion (as long as you keep both read and
write buffers around and don't examine the read buffer until it is complete).
See the Operation Scope section for information on which operations are in scope
for \mintcinline{H5Oflush_async}.

Close operations such as \mintcinline{H5Gclose()} or \mintcinline{H5Fclose()}
will only complete once all previously issued operations on their object, file
or attribute complete. Therefore, the non-async versions of these operations
will block until these operations are complete. Asynchronous versions of all
close operations are available for non-blocking close.

\subsubsection {Parallel Considerations}

Parallel collective operations add another constraint on asynchronous
operations. Because asynchronous MPI operations must be strictly ordered, all
collective HDF5 operations are strictly ordered with respect to each other when
executed with more than one rank, and no two can execute at the same time.
Non-collective operations are not affected by this, and may execute
simultaneouly with collective operations.

\subsubsection {Operation Scope}

All operations exist in an operation pool at attribute, object, file, or global
scope. Operations are generally placed in the operation pool of the parent
object of the operation. For example, dataset creates are placed in the object
scoped pool of the parent group, attribute creates are placed in the object
scoped pool of the parent object, and attribute writes are placed in the
attribute scoped pool of the attribute. Operations that have multiple parent
objects are placed in the file pool if the objects are in the same file, and in
the global pool if they are in different files.

All operations placed in a pool will be executed after all previously issued
operations at a different scope in a location that contains, or is contained in,
the pool for the original operation. For example, an operation in an attribute's
pool will execute after all previously issued operations in the pools for the
attribute's parent object, its file, or the global pool. It is not affected by
operations in a different object or file. Operations in the global pool will
execute after all previously issued operations not in the global pool. Keep in
mind can will serialize operations if, for example, the app switches between
attribute and object operations (though they will remain asynchronous).

\subsection{Building HDF5 DAOS VOL connector applications}

Assuming an \acrshort{hdf5} application has been written following the instructions in the previous section, the application should be built as normal for any other
\acrshort{hdf5} application. However,
if the \dvc{} is not being dynamically loaded, the steps in the following
section are required to build the application.

\subsubsection{Without the DAOS VOL connector as a dynamically-loaded plugin}

To link in the required libraries, the compiler will likely require the
additional linker flags:

\begin{verbatim}
-lhdf5_vol_daos -luuid
\end{verbatim}

However, these flags may vary depending on platform, compiler and installation
location of the \dvc{}. It is highly recommended that compilation
of \acrshort{hdf5} \dvc{} applications be done using either the
\texttt{h5cc/h5pcc} script included with \acrshort{hdf5} distributions, or CMake,
pkg-config, as these will manage linking with the \acrshort{hdf5} library.

If \acrshort{hdf5} was built using autotools, this script will be called \texttt{h5pcc} and
may be found in the \texttt{bin} directory of the \acrshort{hdf5} installation. If \acrshort{hdf5}
was built with CMake, this script will simply be called \texttt{h5cc} and can
be found in the same location. The above notice about additional library
linking applies to usage of \texttt{h5cc/h5pcc}. For example:
\begin{verbatim}
h5cc/h5pcc -lhdf5_vol_daos -luuid my_application.c -o my_application
\end{verbatim}

\subsection{Running HDF5 DAOS VOL connector applications}
\label{running_daos_vol_apps}

Running applications that use the \dvc{} connector requires access to a \acrshort{daos}
server. Refer to
\href{https://daos-stack.github.io/admin/installation/}{DAOS Software Installation}
for more information on the setup process for this. For the \dvc{}
to correctly interact with a \acrshort{daos} server instance, the server must be \hyperref[sec:daos_serv_start]{running} and it must be passed the UUID of the
\acrshort{daos} pool to use and a list of \acrshort{daos} pool service list ranks, as detailed in the
following sections.

\subsubsection{Starting the DAOS Server}
\label{sec:daos_serv_start}

Instructions for starting a \acrshort{daos} Server can be found in the \href{https://daos-stack.github.io/admin/deployment/#server-startup}{DAOS Documentation}.

\subsubsection{With the DAOS VOL connector as a dynamically-loaded plugin}

If the \dvc{} is dynamically loaded by \acrshort{hdf5}, the \acrshort{daos} pool UUID and
\acrshort{daos} pool service rank list are passed via the two environment variables below.

\begin{verbatim}
DAOS_POOL - The UUID of the DAOS pool to use.

DAOS_SVCL - A comma-separated list of server ranks used for
            daos_pool_connect(). Generated from daos_pool_create().
\end{verbatim}

\subsubsection{Without the connector as a dynamically-loaded plugin}

If the \dvc{} is not being dynamically loaded, the \acrshort{daos} pool UUID
and \acrshort{daos} pool service rank list should be passed via the call to
\hyperref[ref:h5daos_init]{H5daos\_init()} within the application.

\subsubsection{Example Applications}

Some of the example C applications which are included with \acrshort{hdf5} distributions have been adapted to work with the \dvc{} and are included under the top-level \href{https://bitbucket.hdfgroup.org/projects/HDF5VOL/repos/daos-vol/browse/examples}{\texttt{examples}} directory in the \dvc{} source root directory. The built example applications can be run from the \texttt{bin} directory inside the build directory.

In addition to these examples, the \href{https://bitbucket.hdfgroup.org/projects/HDF5VOL/
repos/daos-vol/browse/test}{\texttt{test/vol}} directory contains several test
files, each containing test functions that are examples of \acrshort{hdf5} applications in
miniature, focused on a particular behavior. These mini-application tests cover a moderate 
amount of \acrshort{hdf5}'s public API functionality and should be a good indicator of
whether the \dvc{} is working correctly in conjunction with a
running \acrshort{daos} API-aware instance. Note that these tests currently rely on \acrshort{hdf5}'s
dynamically-loaded \vc{} capabilities in order to run with the \dvc{}.

\end{document}
